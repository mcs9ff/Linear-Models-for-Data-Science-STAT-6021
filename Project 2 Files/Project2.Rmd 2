---
title: "Project 2 - STAT 6021"
author: "Sirish Desai, Kevin Kuc, Suraj Kunthu, Matt Scheffel"
date: "2022-12-12"
output: pdf_document
---

# Spotify Music Analysis

## Top 50 songs of 2019

<https://www.kaggle.com/datasets/leonardopena/top50spotify2019>\

```{r}
Data <- read.csv("top50.csv", header = TRUE)
head(Data)
Data <- Data[-1] # eliminate unnecessary index column (R already captures this)
names(Data) <- c("Track Name", "Artist Name", "Genre", "Beats Per Minute", "Energy", "Danceability", "Loudness", "Liveness", "Valence", "Length", "Acousticness", "Speechiness", "Popularity")
head(Data)
```
# Linear Regression
## Exploratory Data Analysis
Identified the following variables for analysis:

- `Beats Per Minute`

- `Loudness`

- `Length`

- `Speechiness`

- `Liveness` 

- Response: `Popularity`

```{r}
library(tidyverse)
pairs(Data[4:13], lower.panel = NULL, main = "Scatterplot of Quantitative Variables")
cor(Data[4:13])
```

Comments:

With `Popularity` as the response:

- Strongest Correlation & Most Negative:

  -  `Valence`

- Most Positive Correlation:

  -  `Speechiness`
  
```{r}
ggplot(Data, aes(x = Speechiness, y = Popularity)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) + 
  labs(x = "Speechiness", y = "Popularity", title = "Scatterplot of Speechiness vs Popularity")

ggplot(Data, aes(x = Loudness, y = Popularity)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) + 
  labs(x = "Loudness (dB)", y = "Popularity", title = "Scatterplot of Loudness vs Popularity")

ggplot(Data, aes(x = Length, y = Popularity)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) + 
  labs(x = "Length (seconds)", y = "Popularity", title = "Scatterplot of Song Length vs Popularity")

ggplot(Data, aes(x = `Beats Per Minute`, y = Popularity)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) + 
  labs(x = "Beats per Minute (bpm)", y = "Popularity", title = "Scatterplot of Beats per Minute vs Popularity")

ggplot(Data, aes(x = Valence, y = Popularity)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) + 
  labs(x = "Valence", y = "Popularity", title = "Scatterplot of Valence vs Popularity")

ggplot(Data, aes(x = Liveness, y = Popularity)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) + 
  labs(x = "Liveness", y = "Popularity", title = "Scatterplot of Liveness vs Popularity")

ggplot(Data, aes(x = Energy, y = Popularity)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) + 
  labs(x = "Energy", y = "Popularity", title = "Scatterplot of Energy vs Popularity")

ggplot(Data, aes(x = Danceability, y = Popularity)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) + 
  labs(x = "Danceability", y = "Popularity", title = "Scatterplot of Danceability vs Popularity")

ggplot(Data, aes(x = Acousticness, y = Popularity)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) + 
  labs(x = "Acousticness", y = "Popularity", title = "Scatterplot of Acousticness vs Popularity")
```

## Choosing Predictors
```{r}
library(leaps)
allreg <- regsubsets(Popularity ~., data = Data[4:13], nbest = 2)
summary(allreg)
```

Adjusted $R^2$
```{r}
which.max(summary(allreg)$adjr2)
coef(allreg, which.max(summary(allreg)$adjr2))
```
Mallow’s $C_p$
```{r}
which.min(summary(allreg)$cp)
coef(allreg, which.min(summary(allreg)$cp))
```

BIC
```{r}
which.min(summary(allreg)$bic)
coef(allreg, which.min(summary(allreg)$bic))
```

Based on the results above, Model 3 is best for $R^2$ and Mallow's $C_p$ with the predictors `Valence` and `Speechiness` The regression equation for Model 3 is:

$$ y = 89.74256882 -0.06151631 x_{Valence} +0.08944084 x_{Speechiness} $$


Based on the results above, Model 1 is best for BIC with the predictor `Valence` The regression equation for Model 1 is:

$$ y = 90.98871799 -0.06389593 x_{Valence} $$
```{r}
## intercept only model
regnull <- lm(Popularity ~ 1, data = Data[4:13])
## model with all predictors
regfull <- lm(Popularity ~ ., data = Data[4:13])
```


### Forward Selection

```{r}
step(regnull, scope=list(lower=regnull, upper=regfull), direction="forward")
```


### Backward Elimination

```{r}
step(regfull, scope=list(lower=regnull, upper=regfull), direction="backward")
```

### Stepwise Regression
```{r}
step(regnull, scope=list(lower=regnull, upper=regfull), direction="both")
```

## Model with Predictors
```{r}
# Check for Multicollinearity
library(faraway)
result_all <- lm(Popularity ~ ., data = Data[4:13])
vif(result_all)
```
Multicollinearity does not exist in this data.

```{r}
result <- lm(Popularity ~ Valence + Speechiness, data = Data[4:13])
summary(result)
```

`Speechiness` p-value is greater than 0.05. Is it insignificant?


$$H_0: \beta_1 = 0; H_A: \beta_1 \neq 0$$
Compare 1-predictor vs 2-predictor model:
```{r}
reduced <- lm(Popularity ~ Speechiness, data = Data[4:13])
anova(reduced, result)
```

From the partial F test, the p-value is less than 0.05, so we reject the null hypothesis at 0.05 significance level. This means we should keep the model with the two predictors: `Valence` & `Speechiness`

## MLR Model

$$ y = 89.74257 - 0.06152 x_{Valence} + 0.08944 x_{Speechiness} $$
```{r}
press <- function(regmodel) {
  sum( (regmodel$residuals) / (1 - lm.influence(regmodel)$hat) )**2 
}
press(result)
```

```{r}
##Find SST
anova_result<-anova(result) 
SST<-sum(anova_result$"Sum Sq") #
#R2 pred
Rsq_pred<-1-press(result)/SST 
Rsq_pred
```

## Transformations

```{r}
# Check Response
Data$yhat <- result$fitted.values
Data$res <- result$residuals
ggplot(Data, aes(x = yhat, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red") +
  labs(x = "Fitted y", y = "Residuals", title = "Residual Plot")
```

Other than 1 outlier, Assumptuions 1 & 2 appear to be me. No transformations on response necessary.

```{r}
# Partial Residual of Valence
result.Popularity.Valence <- lm(Popularity ~ Speechiness, data = Data[4:13])
result.Valence <- lm(Valence ~ Speechiness, data = Data[4:13])

res.Popularity.Valence <- result.Popularity.Valence$residuals
res.Valence <- result.Valence$residuals

##partial residual plot for Valence
plot(res.Valence, res.Popularity.Valence, main="Partial Residual Plot for Valence")
##overlay regression line
abline(lm(res.Popularity.Valence ~ res.Valence), col="red")
```
The residuals are evenly scattered across the regression line. The partial residual plot for Valence informs us that a linear term for Valence will be appropriate when Speechiness is already in the model, and that the estimated coefficient for Valence would be negative in the MLR model with Valence and Speechiness as predictors.


```{r}
# Partial Residual of Speechiness
result.Popularity.Speechiness <- lm(Popularity ~ Valence, data = Data[4:13])
result.Speechiness <- lm(Speechiness ~ Valence, data = Data[4:13])

res.Popularity.Speechiness <- result.Popularity.Speechiness$residuals
res.Speechiness <- result.Speechiness$residuals

##partial residual plot for Speechiness
plot(res.Speechiness, res.Popularity.Speechiness, main="Partial Residual Plot for Speechiness")
##overlay regression line
abline(lm(res.Popularity.Speechiness ~ res.Speechiness), col="red")
```
The residuals are not evenly scattered across the regression line. The partial residual plot for Speechiness informs us that a linear term for Speechiness is not appropriate when Valence is already in the model. A transformation may be necessary. Let us try a log transformation.

```{r}
result.Popularity.logSpeechiness <- lm(Popularity ~ Valence, data = Data[4:13])
result.logSpeechiness <- lm(log(Speechiness) ~ Valence, data = Data[4:13])

res.Popularity.logSpeechiness <- result.Popularity.logSpeechiness$residuals
res.logSpeechiness <- result.logSpeechiness$residuals

##partial residual plot for x2
plot(res.logSpeechiness, res.Popularity.logSpeechiness, main="Partial Residual Plot for log(Speechiness)")
##overlay regression line
abline(lm(res.Popularity.logSpeechiness ~ res.logSpeechiness), col="red")
```
After the log transformation of Speechiness, now the residuals are evenly scattered across the regression line. The partial residual plot for Speechiness informs us that a linear term for Speechiness will be appropriate when Valence is already in the model, and that the estimated coefficient for Speechiness would be positive in the MLR model with Valence and log(Speechiness) as predictors.


```{r}
result1<- lm(Popularity ~ Valence + log(Speechiness), data = Data[4:13])
summary(result1)
```

## MLR Model Diagnostics
```{r}
##critical value using Bonferroni procedure
n <- dim(Data)[1]
p <- 3
crit<-qt(1-0.05/(2*n), n-1-p)
##externally studentized residuals 
ext.student.res<-rstudent(result1) 
##identify 
ext.student.res[abs(ext.student.res)>crit]
```
An outlier was found at item 26, -4.278445: "If I Can't Have You by Shawn Mendes"

```{r}
##leverages
lev <- lm.influence(result1)$hat ##identify
lev[lev>2*p/n]
```
1 songs that has high leverage,
30: "QUE PRETENDES by J Balvin"

```{r}
DFFITS <- dffits(result1)
DFFITS[abs(DFFITS) > 2*sqrt(p/n)]
```
Songs 16 and 26 are influential in terms of $DFFITS_i$.

16: "No Guidance (feat. Drake) by Chris Brown"
26: "If I Can't Have You by Shawn Mendes"

```{r}
DFBETAS <- dfbetas(result1)
abs(DFBETAS) > 2/sqrt(n)
```

Song 1 is influential in terms of $\beta_2$: "Senorita by Shawn Mendes"
Song 10 is influential in terms of $\beta_2$: "bad guy by Billie Eilish"
Song 16 is influential in terms of the Intercept &  $\beta_1$: "No Guidance (feat. Drake) by Chris Brown"
Song 18 is influential in terms of $\beta_1$: "Sunflower - Spider-Man: Into the Spider-Verse by Post Malone"
Song 26 is influential in terms of the Intercept, $\beta_1$, & $\beta_2$: "If I Can't Have You by Shawn Mendes"
Song 39 is influential in terms of $\beta_1$: "Sucker by Jonas Brothers"

```{r}
COOKS<-cooks.distance(result1)
COOKS[COOKS>qf(0.5,p,n-p)]
```

There are no songs that are influential in terms of Cook’s distance.

```{r}
acf(result1$residuals)
```

```{r}
qqnorm(result1$residuals)
qqline(result1$residuals, col="red")
```
## Final MLR Model

Our model:

$$ y = 89.74257 - 0.06152 x_{Valence} + 0.08944 x_{Speechiness}^* $$
$$ x_{Speechiness}^* = \log(x_{Speechiness})$$

# Logistic Regression
## Creating Binary Variable

```{r}
library(stringr)
word <- "pop" # word to search for
Data2 <- data.frame(Data[1:13], "PopCheck" = str_detect(Data$Genre, word)) # str_detect() will look for the word "Pop" in the column and return a Boolean
Data2$PopCheck <- factor(Data2$PopCheck)
# class(Data2$PopCheck)
levels(Data2$PopCheck) <- c("NotPop", "Pop")
head(Data2)
```

```{r}
table(Data2$PopCheck)
```

## Exploratory Data Analysis
```{r}
ggplot(Data2, aes(x = Beats.Per.Minute, fill = PopCheck)) +
  geom_boxplot() +
  labs(x = "Beats Per Minute (bpm)", title = "Box Plot of Beats per Minute in Pop and non-Pop songs")

ggplot(Data2, aes(x = Loudness, fill = PopCheck)) +
  geom_boxplot() +
  labs(x = "Loudness (dB)", title = "Box Plot of Loudness in Pop and non-Pop songs")

ggplot(Data2, aes(x = Length, fill = PopCheck)) +
  geom_boxplot() +
  labs(x = "Length (seconds)", title = "Box Plot of Length in Pop and non-Pop songs")

ggplot(Data2, aes(x = Speechiness, fill = PopCheck)) +
  geom_boxplot() +
  labs(x = "Speechiness", title = "Box Plot of Speechiness in Pop and non-Pop songs")

ggplot(Data2, aes(x = Liveness, fill = PopCheck)) +
  geom_boxplot() +
  labs(x = "Liveness", title = "Box Plot of Liveness in Pop and non-Pop songs")

ggplot(Data2, aes(x = Valence, fill = PopCheck)) +
  geom_boxplot() +
  labs(x = "Valence", title = "Box Plot of Valence in Pop and non-Pop songs")

ggplot(Data2, aes(x = Energy, fill = PopCheck)) +
  geom_boxplot() +
  labs(x = "Energy", title = "Box Plot of Energy in Pop and non-Pop songs")

ggplot(Data2, aes(x = Danceability, fill = PopCheck)) +
  geom_boxplot() +
  labs(x = "Danceability", title = "Box Plot of Danceability in Pop and non-Pop songs")

ggplot(Data2, aes(x = Acousticness, fill = PopCheck)) +
  geom_boxplot() +
  labs(x = "Acousticness", title = "Box Plot of Acousticness in Pop and non-Pop songs")

ggplot(Data2, aes(x = Popularity, fill = PopCheck)) +
  geom_boxplot() +
  labs(x = "Popularity", title = "Box Plot of Popularity in Pop and non-Pop songs")
```

```{r}
# Create new data frame without `Track.Name`, `Artist.Name`, `Genre`
Data3 <- subset(Data2, select = -c(Track.Name, Artist.Name, Genre) )
```

```{r}
## 80-20 split
set.seed(0)
sample <- sample.int(nrow(Data3), floor(.80*nrow(Data3)), replace = F)
train <- Data3[sample, ]
test <- Data3[-sample, ]
```

### 10 Predictor Model
```{r}
result2 <- glm(PopCheck ~ ., family="binomial", data = train)
summary(result2)
```

Automatic Model Selection Search:
```{r}
## intercept only model
regnull2 <- glm(PopCheck ~ 1, family="binomial", data = train)
## model with all predictors
regfull2 <- glm(PopCheck ~ ., family="binomial", data = train)
```

### Forward Selection
```{r}
step(regnull2, scope=list(lower=regnull2, upper=regfull2), direction="forward")
```

### Backward Elmination
```{r}
step(regfull2, scope=list(lower=regnull2, upper=regfull2), direction="backward")
```

### Stepwise Regression
```{r}
step(regnull2, scope=list(lower=regnull2, upper=regfull2), direction="both")
```

`Popularity` & `Speechiness` for 3 out of 3 Automated Search Procedures

### Predictors
```{r}
result2_model <- glm(PopCheck ~ Speechiness + Popularity, family = "binomial", data = train)
summary(result2_model)
```

`Speechiness` p-value is greater than 0.05. Is it insignificant?

$$H_0: \beta_1 = 0; H_A: \beta_1 \neq 0$$
Compare 1-predictor vs 2-predictor model:
```{r}
deltaG2 <- result2_model$null.deviance-result2_model$deviance
deltaG2
```

```{r}
1-pchisq(deltaG2,2)
```

```{r}
reduced1 <- glm(PopCheck ~ Popularity, family="binomial", data = train)
deltaG2_partial <- reduced1$deviance - result2_model$deviance
deltaG2_partial
```

```{r}
1-pchisq(deltaG2_partial, 1)
```

The test statistic is 5.460793 with a p-value that is greater than 0.05. So we reject the null hypothesis, our model is not useful -- we go with the 1-predictor model: `Popularity`.


### Model

$$ \log{(\frac{\hat{\pi}}{1-\hat{\pi}})} =  20.3225399 - 0.2361914 x_{Popularity}$$
####  Model Evaluation

```{r}
library(ROCR)
preds <- predict(reduced1, newdata = test , type="response")
rates <- prediction(preds, test$PopCheck)
roc_result<-performance(rates, measure="tpr", x.measure="fpr")
plot(roc_result, main="ROC Curve for Spotify Data Set")
lines(x = c(0,1), y = c(0,1), col="red")
```
```{r}
auc1<-performance(rates, measure = "auc")
auc1@y.values
```

Since the AUC is above 0.5, our logistic regression performs better than random guessing on the test data.

```{r}
logtable1 <- table(test$PopCheck, preds > 0.5)
logtable1
```

The Accuracy is:
```{r}
sum(logtable2[1], logtable2[4])/sum(logtable2[1:4])
```

The FPR is:
```{r}
logtable1[3] / sum(logtable1[1], logtable1[3])
```

The FNR is:
```{r}
logtable1[2] / sum(logtable1[2], logtable1[4])
```
The error rate is:
```{r}
 logtable1[2]/sum(logtable1[1], logtable1[2])
```


```{r}
test <- data.frame(test,preds)
ggplot(test,aes(x=preds))+
  geom_density()+
  labs(title="Density Plot of Predicted Probs")
```


Change Threshold

```{r}
logtable2 <- table(test$PopCheck, preds > 0.25)
logtable2
```

The Accuracy is:
```{r}
sum(logtable2[1], logtable2[4])/sum(logtable2[1:4])
```

The FPR is:
```{r}
logtable2[3] / sum(logtable2[1], logtable2[3])
```

The FNR is:
```{r}
logtable2[2] / sum(logtable2[2], logtable2[4])
```
The error rate is:
```{r}
sum(logtable2[3], logtable2[2])/sum(logtable2[1:4])
```

## Final LogReg Model

$$ \log{(\frac{\hat{\pi}}{1-\hat{\pi}})} =  20.3225399 - 0.2361914 x_{Popularity}$$

